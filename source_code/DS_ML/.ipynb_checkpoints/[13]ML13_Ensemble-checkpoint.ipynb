{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024f51bf",
   "metadata": {},
   "source": [
    "#### https://www.kaggle.com/uciml/mushroom-classification\n",
    "\n",
    "- 앙상블(Ensemble) : 결정트리 기반 알고리즘 \n",
    "    - 여러 모델을 결합해서 최종 예측 값을 생성(다수결, )\n",
    "    - 각 모델은 상관관계가 적어야 한다\n",
    "- 앙상블 종류\n",
    "1. Voting : \n",
    "    - 서로 다른 알고리즘 가진 분류기를 결합, 사이킷런은 VotingClassifier 클래스를 제공함\n",
    "    - <1> 하드보팅(Hard Voting) : 분류기들이 예측한 결과 값을 다수결로 결정\n",
    "    - <2> 소프트 보팅(Soft Voting) : 각 분류기들이 예측값을 확률로 구하면 이를 평균 내어 확률이 가장 높은 값을 결과 값으로 결정\n",
    "\n",
    "2. Bagging : \n",
    "    - 같은 유형의 알고리즘을 결합, 데이터 샘플링시 서로 다르게 가져가면서 학습, RandomForest 가 대표적, Bootstrapping Aggregation 줄임말\n",
    "    - ( Bootstrapping : 여러개의 데이터 세트를 중첩되게 분리하는 분할 방식 )\n",
    "    - 과대적합을 억제한다 variant를 낮춘다\n",
    "    - 여러 개의 모델에서 하나의 모델을 샘플로 추출한다고 가정해 보자 \n",
    "        - 각 모델의 분산은 평균을 나타내기 때문에 최종 모델의 분산보다 값이 떨어진다 그래서 variant를 낮추는 역할을 하게 된다\n",
    "    - 결정트리는 데이터의 분할이 다르거나 약간의 학습 데이터가 차이가 있으면 결과가 바뀐다\n",
    "        - bagging을 사용해서 variant를 낮추고 과적합을 낮춰서 해결한다\n",
    "    - 병렬 복수 모델\n",
    "    \n",
    "3. Boosting : \n",
    "    - 여러개의 분류기가 순차적으로 학습하면서 가중치를 부스팅한다, XGBoost(캐글 대회 상위 석권),LightGBM\n",
    "    - 학습시에는 부트스트랩 하지 않고 학습 데이터로 학습시킨다\n",
    "        - 단, 가중치가 있기 때문에 학습데이터를 그대로 사용하는 것은 아니다라고 해석한다\n",
    "    - 특징 : variant,  bias 모두 낮춘다\n",
    "    - 직렬 복수 모델\n",
    "\n",
    "4. Stacking 기법 : \n",
    "    - 위의 여러개의 개별적 모델을 학습하여 예측한 데이터 세트를 사용하여 최종 메타 모델이 학습 예측\n",
    "    - 복수 단계로 나누어서 학습 또는 예측하는 기법\n",
    "    - 학습 데이터를 복수 모델로 구축( 배깅 또는 부스팅이르로 앙상블 학습 모델 결과)\n",
    "    - 각 모델의 예측값(출력값): 특징량을 학습 데이터로 추가 모델을 만들어 실행\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor  \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "- 정리\n",
    "- 앙상블학습은 서로 상관 관계가 약한 모델학습기를 조합해서 높은 정밀도를 구축하는 알고리즘\n",
    "- 배깅\n",
    "    - 부트스트랩을 이용해서 추출한 여러 데이터 그룹에 대한 학습모델을 결합(병렬)하는 기술이다\n",
    "    - variant를 억제할 수 있으므로 일반적으로 variant가 높은 결정트리 알고리즘에 사용된다\n",
    "- 부스팅\n",
    "    - 모델이 예측을 잘못된 데이터에 가중치를 부여하고 반복적으로 학습하여 궁극적으로 여러 모델에 가중치를 부여하는 예측 알고리즘이다\n",
    "    - 부스팅을 사용하면 variant와 bias를 모두 낮출 수 있고 배깅보다 높은 정확도를 기대 할 수 있다\n",
    "- 스태킹\n",
    "    - 복수의 모델의 예측치를 새로운 특징량으로 다른 모델을 학습시켜 예측하는 알고리즘이다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0a1f6",
   "metadata": {},
   "source": [
    "#### 랜덤포레스트(RandomForest)\n",
    "- 의사 결정 트리 기반(Decision Tree) 기반 분류 알고리즘\n",
    "    - Decision Tree는 분류 / 회귀 다 있음\n",
    "- 앙상블(Ensemble), 같은 결정트리를 여러개 사용, 비교적 빠른 수행\n",
    "- 현재의 랜덤 포레스트의 개념은 레오 브레이먼(Leo Breimen)의 논문에서 만들어짐, \n",
    "    - 이 논문은 랜덤 노드 최적화(Randomized Node Optimization,RNO)와 \n",
    "    - 배깅(bagging)을 결합한 방법과 같은 CART(Classification And Regression Tree)를 사용해 상관관계가 없는 트리들로 포레스트를 구성하는 방법을 제시했다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a6c4d",
   "metadata": {},
   "source": [
    "- 분류기준 \n",
    "    - 불순도 / 불확실성 : \n",
    "        - DecisionTree 에서 분기를 정하는 기준\n",
    "            - 기준값 : 분기 후 하위 노드내에서 동질성이 있는 하위노드간의 이질성이 커지도록 선택\n",
    "            - 동질성이 증가하면 순도(purity)가 증가하고 불순도가 감소한다\n",
    "            - 이질성이 증가하면 순도가 감소하고 불순도가 증가한다\n",
    "            - 정보 획득(infomation gain) : 순도가 증가하고 불확실성이 감소함\n",
    "            - 순도(purity) 계산 방법 : 엔트로피 지수, 지니계수, 카이제곱 통계량, 분산 감소량 등\n",
    "            - ML : 엔트로피 지수, 지니 계수\n",
    "                - 예시) 사슴, 학, 소, 사슴, 학, 학, 사슴, 기린 8개\n",
    "                - [높은 이질성, 낮은 순수도]\n",
    "                - 지니계수 G = 1-(3/8)**2 - (3/8)**2 - (1/8)**2 - (1/8)**2  = 0.69\n",
    "                - [낮은 이질성, 높은 순수도]\n",
    "                - 지니계수 G = 1-(7/8)**2  - (1/8)**2  = 0.24\n",
    "    - 엔트로피 지수 : \n",
    "        - 무질서도를 정량화해서 표현한 값\n",
    "        - 어떤 집합의 엔트로피가 높을수록 그 집단의 특징을 찾는 것이 어렵다\n",
    "    - 지니 계수 \n",
    "        - 집합에 이질적인 것이 얼마나 섞여있는지 측정하는 지표 \n",
    "        - 어떤 집합에서 하나를 무작위로 뽑아서 클래스를 추정할 때 틀릴 확률\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d31d2b",
   "metadata": {},
   "source": [
    "$$G=\\sum^{K}_{k=1}p(k)(1-p(k))$$\n",
    "$$G=\\frac{1}{4}(1-\\frac{1}{4})+\\frac{3}{4}(1-\\frac{3}{4})=0.375$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26ed88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier() \n",
    "# # 계산 복잡 성능 대비 예측률이 좋다 - 지도 분류학습 (시장조사, 광고조사, 의학연구, 품질관리)\n",
    "# RandomForestClassifier(\n",
    "#     n_estimators=100, (결정트리의 갯수, default=100, 많을 수록 좋은 성능을 기대할수 있지만\n",
    "#                         속도가 느려진다)\n",
    "#     criterion='gini',\n",
    "#     max_depth=None,   (트리의 최대 깊이, 결정트리의 파라메터와 동일)\n",
    "#     min_samples_split=2,(노드를 분할하기 위한 최소한의 샘플 수,default=2,과적합 제어에 사용,결정트리의 파라메터와 동일)\n",
    "#     min_samples_leaf=1,(leaf 노드가 되기 위한 최소한의 샘플 수,default=1,결정트리의 파라메터와 동일)\n",
    "#     min_weight_fraction_leaf=0.0,\n",
    "#     max_features='auto', (최적의 분할을 위해 고려할 최대 피쳐 갯수,결정트리의 파라메터와 동일)\n",
    "#     max_leaf_nodes=None, (리프노드의 최대 갯수, 결정트리의 파라메터와 동일)\n",
    "#     min_impurity_decrease=0.0,\n",
    "#     min_impurity_split=None,\n",
    "#     bootstrap=True,\n",
    "#     oob_score=False,\n",
    "#     n_jobs=None,      (병렬처리 CPU의 갯수, -1이면 전체 CPU 모두 사용)\n",
    "#     random_state=None,(랜덤 seed 설정 값)\n",
    "#     verbose=0,\n",
    "#     warm_start=False,\n",
    "#     class_weight=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c904054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "    random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean() > 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08656f",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=iris.feature_names,  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "iris = load_iris()\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(iris.data, iris.target)\n",
    "r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f05b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2b187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
