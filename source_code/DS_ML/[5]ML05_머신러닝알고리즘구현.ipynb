{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333d3f9",
   "metadata": {},
   "source": [
    "### 머신러닝 분류\n",
    "\n",
    "#### [1] 지도학습(Supervised Learning) : 답이 주어진 상태에서 학습\n",
    "* 회귀(Regression)\n",
    "* 분류(Classification)\n",
    "\n",
    "#### [2] 비지도학습(Unsupervised Learning) : 답을 모르고 학습\n",
    "* 군집화(Clustering)\n",
    "* 차원 축소(Dimension Reduction) : PCA(주성분 분석, Pricipal Component Analysis)\n",
    "\n",
    "#### [3] 강화 학습(Reinforcement Learning) : 답을 모르고 있는 상태에서 답을 알아가는 강한 인공지능(자아를 갖음, 인간수준), 게임, 알파고(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fdda45",
   "metadata": {},
   "source": [
    "### 퍼셉트론과 XOR Problem\n",
    "https://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb990a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.5\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계가\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "    \n",
    "print(AND(0,0))    \n",
    "print(AND(0,1))    \n",
    "print(AND(1,0))\n",
    "print(AND(1,1))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.5\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계가\n",
    "        return 1\n",
    "    elif tmp > theta:\n",
    "        return 0\n",
    "    \n",
    "print(NAND(0,0))    \n",
    "print(NAND(0,1))    \n",
    "print(NAND(1,0))\n",
    "print(NAND(1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.4\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계가\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "    \n",
    "print(OR(0,0))    \n",
    "print(OR(0,1))    \n",
    "print(OR(1,0))\n",
    "print(OR(1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단층 퍼셉트론의 한계\n",
    "# XOR Problem : 아무리 학습시켜도 weight을 구할 수 가 없음\n",
    "# def XOR(x1,x2):\n",
    "#     w1,w2,theta = _,_,_\n",
    "#     tmp = w1*x1 + w2*x2\n",
    "#     if tmp <= theta :  # 임계값\n",
    "#         return 0\n",
    "#     elif tmp > theta:\n",
    "#         return 1\n",
    "\n",
    "# print(XOR(0,0))    # 0 \n",
    "# print(XOR(0,1))    # 1\n",
    "# print(XOR(1,0))    # 1\n",
    "# print(XOR(1,1))    # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a9ae5",
   "metadata": {},
   "source": [
    "### 다층퍼셉트론(MLP)으로 XOR Problem 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR Problem : 서로 다른 weight을 갖는 다층신경망을 사용하여 해결\n",
    "def XOR(x1,x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0,0))    # 0 \n",
    "print(XOR(0,1))    # 1\n",
    "print(XOR(1,0))    # 1\n",
    "print(XOR(1,1))    # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37505fd3",
   "metadata": {},
   "source": [
    "### 회귀(Regression)모델\n",
    "#### [1] 선형 회귀(Linear Regression) : 1차 함수, 직선의 방정식\n",
    "#### [2] 가중치(Weight) : 입력변수가 출력에 영향을 미치는 정도를 설정, 기울기 값 , 회귀 계수\n",
    "#### [3] 편향(Bias) : 기본 출력 값이 활성화 되는 정도를 설정, y 절편\n",
    "#### [4] 비용함수(Cost Function) : 2차 함수, 포물선의 방정식, (예측값 - 실제값)^2\n",
    "* cost(비용) = 오차 = 에러 = 손실(loss)\n",
    "* cost(W,b) = (H(x) - y)^2\n",
    "\n",
    "#### [5] 예측(가설,Hypothesis)함수: predict, H(x) : 예측값, y값:답,결정값,target,label, x:입력, 피쳐(feature)\n",
    "* H(X) = W*X + b\n",
    "\n",
    "#### [6] 경사 하강법(Gradient Descent Algorithm)\n",
    "#### : 비용(cost) 이 가장 작은 Weight(가중치) 값을 구하는 알고리즘\n",
    "\n",
    "#### : Opitmizer : 비용함수를 최소화 하는 w와 b를 구하는 최적의 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수의 구현\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]          # 예측함수(방정식)\n",
    "        loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "        c += loss              # c = c + loss\n",
    "    return c/len(x)            # 평균 제곱 오차\n",
    "    \n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "print('w:-1, cost:', cost(x,y,-1)) # hx = [-1,-2,-3], cost: 18.666666666666668\n",
    "print('w:0, cost:', cost(x,y,0)) # hx = [0,0,0], cost: 4.666666666666667\n",
    "print('w:1, cost:', cost(x,y,1)) # hx = [1,2,3], cost: 0.0,  최저점\n",
    "print('w:2, cost:', cost(x,y,2)) # hx = [2,4,6], cost: 4.666666666666667\n",
    "print('w:3, cost:', cost(x,y,3)) # hx = [2,4,6], cost: 18.666666666666668\n",
    "\n",
    "# 비용함수의 시각화 : x축은 weight, y축은 cost로 하는 2차 함수, 포물선의 방정식\n",
    "for k in range(-30,50):\n",
    "    w = k/10\n",
    "    c = cost(x,y,w)\n",
    "#   print(w,c)\n",
    "    plt.plot(w,c,'ro')  # 'r':red , 'o': 점으로 출력\n",
    "\n",
    "plt.title(' ** Cost Function Graph')    \n",
    "plt.xlabel('weight')    \n",
    "plt.ylabel('cost')\n",
    "plt.grid()\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2bc01",
   "metadata": {},
   "source": [
    "### 미분 : 순간 변화량, 기울기, x축으로 1만큼 움직였을 때 y축으로 움직인 거리\n",
    "#### 함수의 미분 공식 정리 :  f(x) = x^n   ====> f'(x) = n*x^(n-1)\n",
    "* y = 3            ===> y' = 0\n",
    "* y = 2*x          ===> y' = 2\n",
    "* y = x^2          ===> y' = 2*x\n",
    "* y = (x + 1)^2    ===> y' = 2**(x + 1) # y = x^2 + 2**x + 1  ===> 2**x + 2\n",
    "\n",
    "* 곱셈 공식 : (a + b)^2 = a^2 + 2**a**b + b^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556911a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 경사 하강법 알고리즘 함수 구현, 미분 적용\n",
    "def gradient_descent(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx - y[k])*x[k]   # 곱하기 2를 생략한 비용함수의 미분\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "        c += loss\n",
    "    return c/len(x)\n",
    "\n",
    "# x = [1,2,3]\n",
    "# y = [1.2,3.2,5.4]\n",
    "\n",
    "# 학습 시작(train,fit) 시작\n",
    "print('--------------------- start learning!!')\n",
    "w , old = 10, 100\n",
    "for k in range(1000):\n",
    "    c = cost(x,y,w)\n",
    "    grad = gradient_descent(x,y,w)\n",
    "    w -= 0.1* grad   # 0.1:학습율(learning rate),하이퍼 파라메터,가중치의 업데이트실행\n",
    "    print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "#   if c == old : # cost의 변화가 없을때\n",
    "    if c >= old and abs(c - old) < 1.0e-15: # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "        break\n",
    "    old = c\n",
    "print('--------------------- end learning!!')\n",
    "print('weight:',w,'train:',k+1,'회')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cost(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]          \n",
    "        loss = (hx - y[k])**2 \n",
    "        c += loss              \n",
    "    return c/len(x)            \n",
    "\n",
    "\n",
    "def gradient_descent(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx - y[k])*x[k]   \n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "        c += loss\n",
    "    return c/len(x)\n",
    "\n",
    "\n",
    "def fit(x,y):\n",
    "    print('--------------------- start learning!!')\n",
    "    w , old = 10, 100\n",
    "    for k in range(1000):\n",
    "        c = cost(x,y,w)\n",
    "        grad = gradient_descent(x,y,w)\n",
    "        w -= 0.1* grad   \n",
    "        print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "   \n",
    "        if c >= old and abs(c - old) < 1.0e-15: \n",
    "            break\n",
    "        old = c\n",
    "    print('--------------------- end learning!!')\n",
    "    return w\n",
    "\n",
    "\n",
    "def predict(x,w):\n",
    "    hx = w*np.array(x)\n",
    "    return list(hx)\n",
    "\n",
    "\n",
    "def get_accuaracy(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    correct = 0\n",
    "    for k,_ in enumerate(y_test) :\n",
    "        if y_test[k] == y_pred[k] :  \n",
    "            correct += 1\n",
    "    accuracy = round(correct/len(y_test),2)  \n",
    "    return accuracy                \n",
    "\n",
    "\n",
    "def get_rmse(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (y_pred[k] - y_test[k])**2  \n",
    "    mse = squared_error/len(y_test)   \n",
    "    rmse = np.sqrt(mse)               \n",
    "    return rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "w = fit(x_train,y_train)  \n",
    "print('weight:',w)  \n",
    "\n",
    "\n",
    "x_pred = [6,7,8,9,10]   \n",
    "y_pred = predict(x_pred,w)\n",
    "print('y_pred:',y_pred)\n",
    "\n",
    "\n",
    "\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "accuracy = get_accuaracy(x_test,y_test,w)\n",
    "print('Accuracy:',accuracy)  \n",
    "\n",
    "\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "rmse = get_rmse(x_test,y_test,w)\n",
    "print('RMSE:',rmse)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a3957",
   "metadata": {},
   "source": [
    "### class를 사용한 Linear Regression 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self,weight=10):\n",
    "        self.w = weight\n",
    "        print('LinearRegression 생성자')\n",
    "        \n",
    " \n",
    "    def cost(self,x,y) :\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w * x[k]     \n",
    "            loss = (hx - y[k])**2  \n",
    "            c += loss\n",
    "        return c/len(x)           \n",
    "\n",
    "\n",
    "    def gradient_descent(self,x,y):\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w*x[k] \n",
    "            loss = (hx - y[k])*x[k]     \n",
    "            c += loss\n",
    "           \n",
    "        return c/len(x)\n",
    "\n",
    "   \n",
    "    def fit(self,x,y):\n",
    "        print('----------- start learning!!')\n",
    "        old =  100\n",
    "        for k in range(1000):\n",
    "            c = self.cost(x,y)\n",
    "            grad = self.gradient_descent(x,y)\n",
    "            self.w -= 0.1*grad \n",
    "            print('[%03d]'%k,'cost:',c,'old:',old,'weight:',self.w)\n",
    "\n",
    "            if c >= old and abs(c - old) < 1.0e-15: \n",
    "                break\n",
    "            old = c\n",
    "        print('----------- end learning!!')  \n",
    "#         return self.w\n",
    "\n",
    "\n",
    "    def predict(self,x):\n",
    "        hx = self.w*np.array(x)\n",
    "        return list(hx)\n",
    "\n",
    "    def get_accuaracy(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        correct = 0\n",
    "        for k,_ in enumerate(y_test) :\n",
    "            if y_test[k] == y_pred[k] :  \n",
    "                correct += 1\n",
    "        accuracy = round(correct/len(y_test),2)  \n",
    "        return accuracy                \n",
    "\n",
    "  \n",
    "    def get_rmse(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        squared_error = 0\n",
    "        for k,_ in enumerate(y_test):\n",
    "            squared_error += (y_pred[k] - y_test[k])**2  \n",
    "        mse = squared_error/len(y_test)   \n",
    "        rmse = np.sqrt(mse)              \n",
    "        return rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "lr = LinearRegression()  \n",
    "lr.fit(x_train,y_train) \n",
    "print('weight:',lr.w)    \n",
    "\n",
    "\n",
    "x_pred = [6,7,8,9,10]  \n",
    "y_pred = lr.predict(x_pred)  \n",
    "print('y_pred:',y_pred)\n",
    "\n",
    "\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = lr.get_accuaracy(x_test,y_test)  \n",
    "print('Accuarcy:',accuracy)  \n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = lr.get_rmse(x_test,y_test)  \n",
    "print('RMSE:',rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba60f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
